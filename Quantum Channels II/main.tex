\documentclass[notoc]{tufte-book}

\usepackage{xcolor}
\definecolor{quantaOrange}{HTML}{fe8604}

\setcounter{tocdepth}{1}

\hypersetup{colorlinks=true,allcolors=quantaOrange}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

%%
% Book metadata
\title{Quantum Channels II}
\date{Data-processing, recovery channels, and quantum Markov chains}
\author[Felix Leditzky]{Felix Leditzky}
\publisher{Compiled by \href{http://www.twitter.com/jacobbeckey}{Jacob L. Beckey}}

%%
% If they're installed, use Bergamo and Chantilly from www.fontsite.com.
% They're clones of Bembo and Gill Sans, respectively.
%\IfFileExists{bergamo.sty}{\usepackage[osf]{bergamo}}{}% Bembo
%\IfFileExists{chantill.sty}{\usepackage{chantill}}{}% Gill Sans

%\usepackage{microtype}

%%
% Just some sample text
\usepackage{lipsum}

%%
% For nicely typeset tabular material
\usepackage{booktabs}

%%
% For graphics / images
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

%%
% Prints argument within hanging parentheses (i.e., parentheses that take
% up no horizontal space).  Useful in tabular environments.
\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

%%
% Prints an asterisk that takes up no horizontal space.
% Useful in tabular environments.
\newcommand{\hangstar}{\makebox[0pt][l]{*}}

%%
% Prints a trailing space in a smart way.
\usepackage{xspace}

%%
% Some shortcuts for Tufte's book titles.  The lowercase commands will
% produce the initials of the book title in italics.  The all-caps commands
% will print out the full title of the book in italics.
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}

\newcommand{\TL}{Tufte-\LaTeX\xspace}

% Prints the month name (e.g., January) and the year (e.g., 2008)
\newcommand{\monthyear}{%
  \ifcase\month\or January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or
  December\fi\space\number\year
}


% Prints an epigraph and speaker in sans serif, all-caps type.
\newcommand{\openepigraph}[2]{%
  %\sffamily\fontsize{14}{16}\selectfont
  \begin{fullwidth}
  \sffamily\large
  \begin{doublespace}
  \noindent\allcaps{#1}\\% epigraph
  \noindent\allcaps{#2}% author
  \end{doublespace}
  \end{fullwidth}
}

% Inserts a blank page
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}

% Typesets the font size, leading, and measure in the form of 10/12x26 pc.
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

% Macros for typesetting the documentation
\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
  \hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
  \texttt{\tuftebs#2}%
  \ifthenelse{\isempty{#1}}%
    {% add the command to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
    }%
    {% add the command and package to the index
      \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
      \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
    }%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

% Generates the index
\usepackage{makeidx}
\makeindex

%%%% Kevin Godny's code for title page and contents from https://groups.google.com/forum/#!topic/tufte-latex/ujdzrktC1BQ
\makeatletter
\renewcommand{\maketitlepage}{%
\begingroup%
\setlength{\parindent}{0pt}

{\fontsize{24}{24}\selectfont\textit{\@author}\par}

\vspace{1.75in}{\fontsize{30}{50}\selectfont\@title\par}

\vspace{0.5in}{\fontsize{14}{14}\selectfont\textsf{\smallcaps{\@date}}\par}

\vfill{\fontsize{14}{14}\selectfont\textit{\@publisher}\par}

\thispagestyle{empty}
\endgroup
}
\makeatother

\titlecontents{part}%
    [0pt]% distance from left margin
    {\addvspace{0.25\baselineskip}}% above (global formatting of entry)
    {\allcaps{Part~\thecontentslabel}\allcaps}% before w/ label (label = ``Part I'')
    {\allcaps{Part~\thecontentslabel}\allcaps}% before w/o label
    {}% filler and page (leaders and page num)
    [\vspace*{0.5\baselineskip}]% after

\titlecontents{chapter}%
    [4em]% distance from left margin
    {}% above (global formatting of entry)
    {\contentslabel{2em}\textit}% before w/ label (label = ``Chapter 1'')
    {\hspace{0em}\textit}% before w/o label
    {\qquad\thecontentspage}% filler and page (leaders and page num)
    [\vspace*{0.5\baselineskip}]% after
%%%% End additional code by Kevin Godby

%% Packages added by Jacob L. Beckey %%

%Formatting numbers
\renewcommand\labelenumi{\theenumi)}

%Boxes for proofs, solutions, etc
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\definecolor{dark-gray}{HTML}{999999}
\definecolor{light-gray}{HTML}{f5f5f5}

%To include sections numbers

\usepackage{amsmath, bm}
\usepackage{units}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{gensymb}
\usepackage{mathtools}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\usepackage{braket}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{colortbl} 
\usepackage{xcolor} 
\usepackage{xfrac}
\usepackage{multicol}
\usepackage[framemethod=TikZ]{mdframed}

\DeclareMathOperator{\rank}{rank}

\newcommand\byLemmaNine{\stackrel{\mathclap{\normalfont\mbox{\tiny{Lemma \ref{lem:lemma-9}}}}}{=}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\setcounter{secnumdepth}{1}
\begin{document}

% Front matter
%\frontmatter

% r.1 blank page
%\blankpage

% r.3 full title page
\maketitle


% v.4 copyright page
\newpage
\begin{fullwidth}
~\vfill
\thispagestyle{empty}
\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}
Copyright \copyright\ \the\year\ \thanklessauthor

\par\smallcaps{\thanklesspublisher}

\par\smallcaps{tufte-latex.googlecode.com}

\par Licensed under the Apache License, Version 2.0 (the ``License''); you may not
use this file except in compliance with the License. You may obtain a copy
of the License at \url{http://www.apache.org/licenses/LICENSE-2.0}. Unless
required by applicable law or agreed to in writing, software distributed
under the License is distributed on an \smallcaps{``AS IS'' BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND}, either express or implied. See the
License for the specific language governing permissions and limitations
under the License.

\par\textit{Last Update: \monthyear}
\end{fullwidth}

% r.5 contents
\tableofcontents


% Start the main matter (normal chapters)
\mainmatter
\chapter{Relative Entropy and Data-Processing}
\label{ch:relative-entropy-data-processing}

\section{Motivation: quantum state discrimination} \label{sec:quantum-state-discrimination}
We begin by considering the task of quantum state discrimination. Assume you are given one of two quantum state $\rho_0,\rho_1$ with equal probability and your goal is to decide which state you got. The strategy will be to perform a measurement\sidenote{In this course, measurements will be described by a positive operator-valued measure (POVM): $\{\Lambda_i\}_{i=1}^k, \Lambda_i \geq 0, \sum_i \Lambda_i = \mathbb{1}_{\mathcal{H}}$. For a given state $\rho$, POVM gives outcome $i$ with probability $p_i = \text{tr}(\Lambda_i \rho)$.} on the unknown state, the outcome of which will determine your guess at which state you were given.

In the problem of state discrimination, we use a POVM \index{POVM} $\Lambda = {\Lambda_0,\Lambda_1}$ with $\Lambda_1 = \mathbb{1}-\Lambda_0$ and $\Lambda_0 \geq 0$. Thus, the outcome $``0''$ corresponds to $\rho_0$ and outcome $``1''$ corresponds to $\rho_1$ with probabilities given by $p_i = \text{tr}(\Lambda_i \sigma)$ and $\sigma \in \{\rho_0,\rho_1\}$.

So, the success probability of correctly identifying the state is given by
\begin{align}
    p_{\text{succ}} (1) &= \frac{1}{2} \text{Pr}(\rho_0 | \rho_0) + \frac{1}{2} \text{Pr}(\rho_1 | \rho_1), \\
    &= \frac{1}{2}(\text{tr}\Lambda_0 \rho_0 + \text{tr} \Lambda_1 \rho_1), \\
    &= \frac{1}{2}(\text{tr}\Lambda_0 \rho_0 + 1 - \text{tr} \Lambda_0 \rho_1), \\
    &= \frac{1}{2} \left(1 + \text{tr}\left[ \Lambda_0 (\rho_0 - \rho_1)\right]\right).
\end{align}
Of course, we want to maximize the success probability with respect to the POVM $\Lambda = \{\Lambda_0, \Lambda_1\}$. That is we want to find
\begin{align}
    p^*_{\text{succ}} = \max_{0\leq \Lambda_0 \leq \mathbb{1}} p_{\text{succ}}(1) = \frac{1}{2} \left(1+\max_{0\leq \Lambda_0 \leq \mathbb{1}} \text{tr}\left[ \Lambda_0(\rho_0-\rho_1)\right] \right).
\end{align}

Recall that the trace norm \index{trace norm} $\| X\|_1 = \text{tr}\sqrt{X^{\dagger}X} = \sum_i s_i (X) $ where the $s_i's$ are the singular values of $X$.\sidenote{That is, they are the eigenvalues of $|X| = \sqrt{X^{\dagger}X}$.} From this norm we can formulate the trace distance Also recall that the trace distance \index{trace distance} between two quantum states $\rho_0,\rho_1$ is given as
\begin{align}
    T(\rho_0,\rho_1) = \frac{1}{2}\|\rho_0 - \rho_1\|_1.
\end{align}
The fact that this distance can be formulated as a maximization over POVMs is the content of our first lemma.

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{lemma}\label{lem:lemma-1}
Let $\rho_0, \rho_1$ be quantum states, then 
\begin{align}\label{eq:trace-distance-max}
\frac{1}{2} \| \rho_0 - \rho_1\|_1 = \max_{0\leq \Lambda \leq \mathbb{1}} \text{tr}\left[\Lambda (\rho_0 - \rho_1)\right]
\end{align}
\end{lemma}
\end{tcolorbox}
\begin{proof}
$\rho_0-\rho_1$ is Hermitian, so we can write is spectral decomposition as $\sum_i \lambda_i \ket{i}\bra{i}$ where $\lambda_i \in \mathbb{R}, \braket{i|j} = \delta_{ij}$. Then, let us define
\begin{align}
    P &= \sum_{i:\lambda_i \geq 0} \lambda_i \ket{i}\bra{i} \geq 0, \\
    Q &= \sum_{i:\lambda_i < 0} (-\lambda_i) \ket{i}\bra{i} \geq 0,
\end{align}
and $P-Q := \rho_0 - \rho_1 $. Then we have\sidenote{Note that here we use $\text{tr}(P-Q) = \text{tr}(\rho_0-\rho_1)=\text{tr}\rho_0 - \text{tr}\rho_1 =0$ implies $\text{tr} P = \text{tr} Q$.}
\begin{align}
    \|\rho_0 - \rho_1\|_1 = \text{tr} |\rho_0 - \rho_1| = \text{tr}|P-Q| = \text{tr}P + \text{tr}Q = 2\text{tr}P,
\end{align}
which implies $\frac{1}{2}\|\rho_0-\rho_1\|_1 = \text{tr}P$. We now want to relate this result to $\text{tr} \left[\Lambda (\rho_0 - \rho_1 )\right]$ for arbitrary $0\leq \Lambda \leq \mathbb{1}$. We write
\begin{align}
    \text{tr}\Lambda (\rho_0 - \rho_1)&= \text{tr}\Lambda(P-Q) \\
    &\leq \text{tr}\Lambda P \\
    &\leq \text{tr}P \\
    &= \frac{1}{2} \|\rho_0 - \rho_1\|_1
\end{align}
This means that $\max_{0\leq \Lambda_0 \leq \mathbb{1}} \text{tr}\Lambda(\rho_0 - \rho_1) \leq \frac{1}{2} \|\rho_0 - \rho_1\|_1$. It remains to be shown that there exists a $\Lambda$ that achieves this maximum. Set $ \Lambda = \Pi_p = \sum_{i:\lambda_i \geq 0} \ket{i}\bra{i}$ (the projector onto the support of $P$). Then, we have 
\begin{align}
    \text{tr} \Pi_p (\rho_0 - \rho_1) &= \text{tr} \Pi_p (P-Q)\\
    &=\text{tr} \Pi_p P - \text{tr} \Pi_p Q \\
    &= \text{tr} P \\
    &= \frac{1}{2} \| \rho_0 - \rho_1\|_1
\end{align}
\end{proof}

We see that the success probability of correctly identifying the state is
\begin{align}
    p_{\text{succ}} &= \frac{1}{2} (1+ \frac{1}{2} \|\rho_0-\rho_1 \|_1).
\end{align}
So, if the trace distance is one (zero) , we see success probability is one (one half). \textit{Thus, the trace distance between two states is a measure of distinguishability. } 

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{proposition}
Let $T(\rho_0, \rho_1) = \frac{1}{2} \|\rho_0 - \rho_1\|_1$. For any two states $\rho_0, \rho_1$ and a quantum channel $\mathcal{N}$, we have
\begin{align}
    T(\rho_0,\rho_1) \geq T(\mathcal{N}(\rho_0),\mathcal{N}(\rho_1)).
\end{align}
\end{proposition}
\end{tcolorbox}

\begin{proof}
We know from Lemma \eqref{lem:lemma-1} that 
\begin{align}
    \frac{1}{2} \|\rho_0 - \rho_1\|_1 = \max_{0 \leq \Lambda \leq 1} \text{tr} \Lambda(\rho_0 - \rho_1).
\end{align}

Let $\Lambda \geq 0$ with $\Lambda \leq \mathbb{1}$ be optimal for $\frac{1}{2}\|\mathcal{N}(\rho_0)-\mathcal{N}(\rho_1)\|_1$:

\begin{align}
    \frac{1}{2}\|\mathcal{N}(\rho_0)-\mathcal{N}(\rho_1)\|_1 &=\text{tr}\Lambda(\mathcal{N}(\rho_0)-\mathcal{N}(\rho_1)), \\
    &= \text{tr} \Lambda \mathcal{N}(\rho_0 - \rho_1), \\
    &= \text{tr} \mathcal{N}^{\dagger}(\Lambda)(\rho_0 - \rho_1). 
\end{align}
\sidenote{Where the last equality holds by definition of adjoint map.}Now, we want to show 1) $\mathcal{N}^{\dagger}(\Lambda) \geq 0$ and 2) $\mathcal{N}^{\dagger} (\Lambda) \leq \mathbb{1}$. 

\begin{enumerate}
    \item The first is easy to see by recalling that the adjoint map is completely positive because all quantum channels are completely positive. Positivity is a weaker condition, so in particular we have
\begin{align}
    \Lambda \geq 0 \implies \mathcal{N}^{\dagger}(\Lambda) \geq 0.
\end{align}

    \item Further, the adjoint map is unital\sidenote{Recall, unitality means $\mathcal{N}(\mathbb{1}) =\mathbb{1}$.}, so we have 
\begin{align}
    \mathcal{N}^{\dagger}(\mathbb{1}-\Lambda) &\geq 0, \\
    \mathcal{N}^{\dagger}(\mathbb{1}) - \mathcal{N}^{\dagger}(\Lambda) &\geq 0, \\
  \mathbb{1}  &\geq \mathcal{N}^{\dagger}(\Lambda).
\end{align}
\end{enumerate}

So, from 1) and 2), we can conclude $\mathcal{N}^{\dagger}(\Lambda)$ is feasible\sidenote{A \href{https://en.wikipedia.org/wiki/Feasible_region}{feasible region} is the set of all possible points of an optimization problem that satisfy the problem's constraints.} in 
\begin{align}\max_{0 \leq K \leq \mathbb{1}} \text{tr} K (\rho_0 - \rho_1).
\end{align}
Now that we know the adjoint is achievable, we have 
\begin{align}
    \frac{1}{2} \|\rho_0 - \rho_1\|_1 &= \max_{0 \leq K \leq \mathbb{1}} \text{tr}K(\rho_0 -\rho_1),\\
    &\geq \text{tr} \mathcal{N}^{\dagger}(\Lambda)(\rho_0 - \rho_1),\\
    &= \frac{1}{2} \|\mathcal{N}(\rho_0)-\mathcal{N}(\rho_1)\|_1. 
\end{align}
\end{proof}

We have thus shown that the trace distance cannot increase when subject to the same noisy process. States will never become more distinguishable after processing. Mathematically, 
\begin{align}\label{eq:trace-distance-data-processing}
    \frac{1}{2} \|\rho_0 - \rho_1\|_1 &\geq \frac{1}{2} \|\mathcal{N}(\rho_0)-\mathcal{N}(\rho_1)\|_1. 
\end{align}

\section{Error analysis and hypothesis testing}
The probability of success can be expressed
\begin{align}
    p_{\text{succ}} &= \frac{1}{2} \text{Pr}(\rho_0 | \rho_0) +\frac{1}{2} \text{Pr}(\rho_1 | \rho_1).
\end{align}
Then, the error probability will be given as 
\begin{align}
    p_{\text{error}} &= 1- p_{\text{succ}} = \frac{1}{2} \left(\text{Pr}(\rho_1|\rho_0)+\text{Pr}(\rho_0|\rho_1)\right).
\end{align}

In hypothesis testing, there is a null hypothesis $H_0 (\rho_0)$ and an alternative hypothesis $H_1 (\rho_1)$. Then, we say a type-1 error\sidenote{Often denoted by the Greek letter $\alpha$.} is committed when we infer $\rho_1$ when you actually have $\rho_0$ (false rejection or false negative). A type-2 error\sidenote{Often denoted by the Greek letter $\beta$.} is when you infer $\rho_0$ when you really have $\rho_1$ (false acceptance or false positive). There then exists a trade-off between these two errors. 
\begin{itemize}
    \item Symmetric hypothesis testing (``Bayesian''): try and minimize the sum of the two errors. This leads to the trace distance as discussed above.
    \item Asymmetric hypothesis testing: assume that type-1 error is constant and small. The question is then, how small can I make the type-2 error under this constraint.\sidenote{In realistic settings we want to avoid false negatives at all costs!}
\end{itemize}

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{definition}
\label{def:errors}
Let $\rho,\sigma$ be two quantum states. Let $\rho$ be the null hypothesis and let $\sigma$ be the alternative hypothesis. Further, let $\Lambda$ with $0\leq \Lambda \leq \mathbb{1}$ be a test operator defining a two-element POVM $\{\Lambda, \mathbb{1}-\Lambda\}$. Then we have the following errors
\begin{align}
    \alpha(\Lambda) &=\text{tr}\rho(\mathbb{1}-\Lambda) \quad \text{ type-1 error}\\
    \beta(\Lambda) &= \text{tr}\sigma \Lambda \quad \text{ type-2 error} 
\end{align}
\end{definition}
\end{tcolorbox}
In an information-theoretic setting, we wish to determine $\rho^{\otimes n}$ versus $\sigma^{\otimes n}$ as $n\rightarrow \infty$. In this case we have\sidenote{Note that $\Lambda_n \in B(\mathcal{H}^{\otimes n}), \Lambda_n \geq 0,$ and $ \Lambda_n \leq \mathbb{1}_{\mathcal{H}}^{\otimes n}$}
\begin{align}
    \alpha_{n}(\Lambda_n) &= \text{tr} \rho^{\otimes n} (\mathbb{1}-\Lambda_n), \\
    \beta_n (\Lambda_n) &= \text{tr}\sigma^{\otimes} \Lambda_n.
\end{align}
Then, for $\epsilon >0$, we define
\begin{align}
    \beta_n^{*} (\epsilon) = \min \{\beta_n (\Lambda_n): 0 \leq \Lambda_n \leq \mathbb{1}, \alpha_n (\Lambda_n) \leq \epsilon\}.
\end{align}
The question is, how does $\beta^{*}_n(\epsilon)$ behave\sidenote{For example, $\beta^{*}_n=f(n) \rightarrow 0$, what is $f$?} as $n \rightarrow \infty$?

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{definition}
\begin{enumerate}
    \item For a linear operator $X\in B(\mathcal{H})$, the support of $X$ is defined as 
    \begin{align}
        \text{supp}X = (\text{ker} X)^{\perp}.
    \end{align}
    If $X$ is Hermitian with spectral decomposition $X=\sum_i \lambda_i \ket{i}\bra{i}$, then $\text{supp}X=\text{span}\{\ket{i}: \lambda_i \neq 0\}.$ The projection onto $\text{supp}X$ is given by
    \begin{align}
        \sum_{i:\lambda_i \neq 0} \ket{i}\bra{i} = \lim_{\alpha \rightarrow 0} X^{\alpha} = X^{0}
    \end{align}
    \item Let $\rho \geq 0$, $\text{tr}\rho = 1$, $\sigma \geq 0$. Then, the \textit{relative entropy}\index{relative entropy} is defined as 
    \begin{align}\label{eq:relative-entropy}
        D(\rho \| \sigma) &= \begin{cases}
        \text{tr}(\rho \log{\rho} - \rho \log{\sigma}) \quad &\text{if } \text{supp}\rho \subseteq \text{supp}\sigma, \\
        \infty \quad &\text{else}.
        \end{cases}
    \end{align}
\end{enumerate}
\end{definition}
\end{tcolorbox}

This definition brings us to the so-called \textit{quantum Stein's lemma}.\sidenote{This result was shown by \href{https://link.springer.com/article/10.1007/BF02100287}{Hiai and Petz} and complemented nicely by \href{https://arxiv.org/pdf/quant-ph/9906090.pdf}{Ogawa and Nagaoka}.} The lemma says that for all $\epsilon > 0$, 
\begin{align}
    \lim_{n\rightarrow \infty} \frac{1}{n} \log{\beta_n^{*} (\epsilon)} = - D(\rho \| \sigma).
\end{align}
Intuitively, we can see $\beta_n^{*} \approx \exp{(-n D(\rho \| \sigma))}$. Thus, we have a measure of distinguishability in the asymmetric setting. The larger $D(\rho \| \sigma)$, the better one can distinguish between $\rho$ and $\sigma$ (decay of $\beta^*_n,$ optimal type-II error if type-I error is bounded and small). Note that because $D(\rho \| \sigma) \neq D(\sigma \| \rho)$, the relative entropy is not a metric in the rigorous sense. However, if $\rho$ and $\sigma$ are quantum states, then $D(\rho \| \sigma) \geq 0$ with equality if and only if $\rho=\sigma$.

\section{Properties of relative entropy}
Measures of distinguishability between quantum states should be monotonic under quantum operations to match the intuition that noise cannot make quantum states more distinguishable. This is summarized in the following theorem\sidenote{This is the data-processing inequality for the quantum relative entropy.}.

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{theorem}\label{thm:data-processing-relative-entropy}
Let $\rho$ be a quantum state, $\sigma \geq 0$, and $\mathcal{N}$ a quantum channel. Then,
\begin{align}
    D(\rho \| \sigma) \geq D(\mathcal{N}(\rho) \| \mathcal{N}(\sigma)).
\end{align}
\end{theorem}
\end{tcolorbox}
We will defer the proof to later and first study the properties of the relative entropy.

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{proposition}\label{prop:properties-of-rel-entropy}
\begin{enumerate}
    \item Let $\rho$ be a classical state, and $\sigma \geq 0$ be classical (diagonal with respect to the same basis). That is, for $\rho = \sum_x p_x \ket{x}\bra{x}$ and $\sigma = \sum_x q_x \ket{x}\bra{x}$, we have
    \begin{align}
        D(\rho \| \sigma) &= \sum_x p_x \log{\frac{p_x}{q_x}} = D(p\|q),
    \end{align}
    where the final quantity is the classical Kullback-Leibler divergence. 
    
    \item Let $\rho,\sigma$ be quantum states, then $D(\rho \| \sigma) \geq 0$, and $D(\rho \| \sigma)=0$ iff $\rho=\sigma$.
    \item $D(\rho \| \sigma) = D(V\rho V^{\dagger} \| V\sigma V^{\dagger})$ for an isometry $V$. 
    
    \item For classical-quantum states $\rho_{xA} = \sum_x p_x \ket{x}\bra{x}_x \otimes \rho^x_A$ and $\sigma_{xA} =\sum_x p_x \ket{x}\bra{x}_X \otimes \sigma^x_A$, we have 
    \begin{align}
        D(\rho_{xA} \| \sigma_{xA}) = \sum_x p_x D(\rho_A^x \| \sigma_A^x).
    \end{align}
    \item Joint convexity: Let $\{\rho_x\}$ be states and $\{\sigma_x\}$ be positive semi-definite operators and $\{\lambda_x \}$ be a probability distribution. Then,
    \begin{align}
        D(\sum_x \lambda_x \rho_x \| \sum_x \lambda_x \sigma_x) \leq \sum_x \lambda_x D(\rho_x \| \sigma_x).
    \end{align}
    
    \item Let $\rho$ be a state and $\sigma,\sigma' \geq 0$ with $\sigma \leq \sigma'$, then \begin{align}D(\rho \| \sigma) \geq D(\rho \sigma').\end{align}
\end{enumerate}
\end{proposition}
\end{tcolorbox}

\begin{proof}
\begin{enumerate}

\item This follows directly from the definition of the relative entropy. 
\item $\rho,\sigma$ are states: use data-processing inequality with respect to $\mathcal{N} = \text{tr}$. We then have
\begin{align}
    D(\rho \| \sigma) &\geq D(\text{tr}\rho \| \text{tr}\sigma), \\
    &= D(1 \| 1), \\
    &= 0.
\end{align}
We will prove that $D(\rho \| \sigma) =0$ iff $\rho=\sigma$ later.

\item Now we wish to prove isometric\sidenote{(Keep in mind, an isometry is a linear map satisfying $V^{\dagger} V = I$).} invariance. The proof will be done in two parts. First, note that $V \cdot V^{\dagger}$ can be viewed as a quantum channel. So, by DPI, we have
\begin{align}
    D(\rho \| \sigma) \geq D(V\rho V^{\dagger} \| V \sigma V^{\dagger}).
\end{align}
Next, define $\Pi = V V^{\dagger}$ as the projection onto the image of $V$. Define $W: B(\mathcal{K}) \rightarrow B(\mathcal{H})$ act as
\begin{align}
    W(X) &= V^{\dagger}XV + \text{tr}\left((\mathbb{I}-\Pi) X\right) \ket{0}\bra{0}, \\
    \text{tr}W(X) &= \text{tr}V^{\dagger}XV + \text{tr}((\mathbb{1}- \Pi)X), \\
    &= \text{tr}X.
\end{align}
Further, note $W(VYV^{\dagger})=Y$. 
This is as completely positive, trace preserving map (as one should check). Then the second step of our proof is 
\begin{align}
    D(\rho \| \sigma) &\geq D(V \rho V^{\dagger} \| V\sigma V^{\dagger}),\\
    &\geq D(W(V \rho V^{\dagger})\| W(V \sigma V^{\dagger})),\\
    &= D(\rho \| \sigma),
\end{align}
which completes the proof.

\item cq-states: $\rho_{xA} = \sum_x p_x \ket{x}\bra{x}_X \otimes \rho_A^x$ = $\bigoplus_x p_x \rho_A^x$. Then, $D(\rho \| \sigma) = \text{tr}\left(\rho (\log \rho - \log \sigma)\right)$. \textbf{Insert diagram showing block diagonal form.} Then, we can have
\begin{align}
    \log{\rho_{xA}} - \log{\sigma_{xA}} &= \sum_x \ket{x}\bra{x}_x \otimes (\log{\rho_A^x} - \log{\sigma_A^x}).
\end{align}
But we know that
\begin{align}
    D(\rho_{xA} \| \sigma_{xA}) &=\text{tr} \rho_{xA} \left(\log{\rho_{xA}} - \log{\sigma_{xA}}\right),\\
    &= \text{tr}[(\sum_x p_x \ket{x}\bra{x}\otimes \rho^x_A)\\
    &\times (\sum_y \ket{y}\bra{y} \otimes (\log \rho_A^y-\log \sigma_A^y))],\\
    &= \sum_x p_x \text{tr}\left(\ket{x}\bra{x} \otimes \rho_A^x (\log{\rho_A^x - \log{\sigma_A^x}})\right),\\
    &= \sum_x p_x D(\rho_A^x \| \sigma_A^x).
\end{align}
This completes the proof. 
\item We wish to show joint convexity. The idea is to use item 4) above with $\rho_{xA} = \sum_x \lambda_x \ket{x}\bra{x} \otimes \rho_{A}^x, \sigma_{xA} = \sum_x \lambda_x \ket{x}\bra{x} \otimes \sigma_A^x$. Then from DPI with respect to $\text{tr}_X$, we get the result we want.

\item Finally, we wish to show that if $\sigma \leq \sigma'$, then $D(\rho \| \sigma) \geq D(\rho \| \sigma')$. Using 4) we can write $
    D(\rho \| \sigma) = D(\rho \otimes \ket{0}\bra{0} \| \sigma \otimes \ket{0}\bra{0} +(\sigma'-\sigma)\otimes \ket{1}\bra{1})$. Then by DPI, we have
    \begin{align}
        D(\rho \| \sigma) \geq D(\rho \| \sigma +\sigma' -\sigma) = D(\rho \|\sigma')
    \end{align}
\end{enumerate}
\end{proof}

Note that the isometric invariance of the relative entropy can be proved directly, without DPI. Let $V$ be an isometry. Then
\begin{align}
    D(V\rho V^{\dagger} \| V \sigma V^{\dagger}) &= \text{tr} V\rho V^{\dagger} (\log V\rho V^{\dagger}-\log V \sigma V^{\dagger}) \\
    &= \text{tr}V\rho V^{\dagger} V(\log \rho - \log \sigma)V^{\dagger} \\
    &= \text{tr} \rho (\log \rho - \log \sigma) \\
    &= D(\rho \| \sigma).
\end{align}
Now although this proof works equally well, using DPI can actually be applied to any ``divergence''. It is in this way more fundamental. Than isometric invariance. Further, joint concavity can be used to prove DPI (this means they are equivalent). So, we want to show $D(\rho_{AB} \| \sigma_{AB}) \geq D(\rho_A \| \sigma_A)$. 

In general, the following holds $D(\rho\otimes \omega \| \sigma \otimes \tau) = D(\rho \| \sigma) + D(\omega \| \tau)$.

Next goal is to prove the data-processing inequality we continue using. First we need some results from operator theory. They will be stated without proof. 

\subsection{Detour 1: Functions on Operators and Operator Convexity}
Let $A \in B(\mathcal{H})$ be Hermitian with spectral decomposition given by
\begin{align}
    A = \sum_i \lambda_i \ket{i}\bra{i}
\end{align}
with $\lambda_i \in \mathbb{R}, \braket{i|j}=\delta_{ij}$. Then, let $f:I \rightarrow \mathbb{R}, I \subseteq \mathbb{R}$, be such that spec$A\subseteq I$, then
\begin{align}
    f(A) = \sum_i f(\lambda_i) \ket{i}\bra{i}.
\end{align}
In words, $f(A)$ is a Hermitian operator with the same eigenbasis as $A$ and spectrum $\{f(\lambda_i)\}_i$.\sidenote{Note that this implies the often used property that $[A,f(A)]=0$.}

As an example, consider the matrix logarithm. Given a state with spectral decomposition $\rho=\sum_i \lambda_i \ket{i}\bra{i} \implies \log{\rho} = \sum_i \log{\lambda_i} \ket{i}\bra{i}$. Another important example is the entropy function\sidenote{Note that because $\lim_{t\rightarrow 0} \eta (t) =0$, we define $0\log 0 := 0$.} $\eta (t) = t\log{t}$. 

Letting $V: \mathcal{H}\rightarrow \mathcal{K}$ be an isometry, $V^{\dagger} V = \mathbb{I}$, then
\begin{align}
    f(VAV^{\dagger})=Vf(A)V^{\dagger}.
\end{align}
We will study functions that are operator convex\sidenote{Recall the partial order ``$\leq$'' on Hermitian operators: $
    A\leq B : \leftrightarrow B-A \geq 0.$ }:
    \begin{align}
        f(\lambda A + (1-\lambda)B) \leq \lambda f(A)+(1-\lambda)f(B), 
    \end{align}
for all $\lambda \in [0,1].$ An immediate result is that every operator convex function is convex as a real function\sidenote{Just take dim$\mathcal{H}=1$}. However, the converse of this statement is \textit{not} true\sidenote{For example, $t \mapsto t^3$ is not operator convex.}. Some examples of operator convex functions include
\begin{enumerate}
    \item $t\mapsto t^p$ for $-1 \leq p \leq 0$ and $1\leq p \leq 2$,
    \item $t\mapsto -t^p$ for $0 \leq p \leq 1$ ,
    \item $t \mapsto -\log{t}$,
    \item $t \mapsto \eta(t)=t\log{t}, \quad \eta(0)=0$.
\end{enumerate}
Finally, though we save the proof for later, we state an important result called the \textit{operator Jensen\sidenote{Named after \href{https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)}{Johan Jensen (1859-1925). }} inequality}:\index{operator Jensen's inequality} Let $f: I \rightarrow \mathbb{R}$ be operator convex, $V: \mathcal{H}\rightarrow \mathcal{K}$ and isometry, $A$ Hermitian with spec$A\subseteq I$. Then, for $A \in B(\mathcal{K})$: 
\begin{align}\label{eq:operator-Jensens}
    f(V^{\dagger}A V) \leq V^{\dagger} f(A) V.
\end{align}

\subsection{Relative Modular Operator}
Fix $A,B \in B(\mathcal{H})$ and define maps $L_A, R_B: B(\mathcal{H})\rightarrow B(\mathcal{H})$:
\begin{align}
    L_A(X) = AX \quad R_B(X) = XB
\end{align}
\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{lemma}
\begin{enumerate}
    \item $[L_A,R_B]=0$
    \item If $A$ is invertible, then $L_A,R_A$ are invertible, and 
    \begin{align}
        L_A^{-1} = L_{A^{-1}}, \quad R^{-1}_A = R_{A^{-1}}
    \end{align}
    \item if $A$ is Hermitian, then so are $L_A,R_A$ with respect to the Hilbert-Schmidt inner product $\langle X,Y \rangle = \text{Tr}(X^{\dagger} Y)$
    \item If $A$ is Hermitian and $f:I\rightarrow \mathbb{R}$ with spec$A\subseteq I$, then $f(L_A), f(R_A)$ are well defined and 
    \begin{align}
        f(L_A)-L_{f(A)}, \quad f(R_A)=R_{f(A)}.
    \end{align}
\end{enumerate}
\end{lemma}
\end{tcolorbox}

\begin{proof}
\begin{enumerate}
    \item \begin{align}
        [L_A,R_B]X&= L_A (R_B X) - R_B (L_A X), \\
        &= L_A( XB) - R_B( AX), \\
        &= AXB -AXB, \\
        &= 0
    \end{align}
    \item By definition
    \item We have $\langle X, L_A(Y) \rangle = \langle L_A^{\dagger}(X),Y\rangle$ and 
    \begin{align}
        \langle X,L_A(Y) \rangle &= \text{Tr} X^{\dagger}A Y, \\
        &= \text{Tr} ((A^{\dagger}X)^{\dagger}Y), \\
        &= \langle A^{\dagger}X,Y \rangle,
    \end{align}
    which implies $L^{\dagger}_A = L_{A^{\dagger}}$. Finally by the Hermiticity of $A$, we have
    \begin{align}
        L^{\dagger}_A = L_A,
    \end{align}
    as desired. The analogous argument works for $R_A$.
    \item Let $A=\sum_{i=1}^d a_i \ket{i}\bra{i}$ where $d=\text{dim}\mathcal{H}$. Then, it follows that $L_A (\ket{i}\bra{j}) = A \ket{i}\bra{j} = a_i \ket{i}\bra{j}$. There are $d$ eigenoperators $\ket{i}\bra{j}$ with eigenvalue $a_i$ (for $j=1,\dots,d$) which implies there are $d^2$ eigenvalues with orthogonal eigenoperators $\ket{i}\bra{j}:$
    \begin{align}
        \langle \ket{i}\bra{j}, \ket{k}\bra{l} \rangle = \delta_{ik} \delta_{jl}.
    \end{align}
    Further, dim$B(\mathcal{H})=d^2$. Thus, we can define $f(L_A)$ through $f(L_A)(\ket{i}\bra{j}) = f(a_i)\ket{i}\bra{j} = L_{f(A)} (\ket{i}\bra{j})$. Since $\{\ket{i}\bra{j}\}_{i,j=1}^d$ is a basis, $f(L_A)=L_{f(A)}$. The same argument holds for $R_A$.
\end{enumerate}
\end{proof}
\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{definition} \index{relative modular operator}
Let $X,Y \in B(\mathcal{H})$ be Hermitian and $Y$ invertible. The relative modular operator $\Delta = \Delta^{X,Y}$ is defined as
\begin{align}
    \Delta^{X,Y} = L_X R_{Y^{-1}} : Z \mapsto XZY^{-1}
\end{align}
\end{definition}
\end{tcolorbox}

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{lemma}\label{lem:lemma-9}
Let $X,Y \geq 0, Y$ invertible, and $\eta (t) = t \log{t}$. Then $\eta (\Delta^{X,Y}) = \Delta^{X,Y}(L_{\log{X}}-R_{\log{Y}})$.
\end{lemma}
\end{tcolorbox}

\begin{proof}
Let $X,Y$ have spectral decompositions\sidenote{Note that $Y$ being invertible impies $y_i>0$.}
\begin{align}
    X&=\sum_i x_i \ket{e_i}\bra{e_i},\\ 
    Y&= \sum_i y_i \ket{f_i}\bra{f_i}.
\end{align}
Then, we have 
\begin{align}
    \Delta^{X,Y}(\ket{e_i} \bra{f_j})&= L_{X} R_{Y^{-1}} (\ket{e_i}\bra{f_j}),\\
    &= X \ket{e_i}\bra{f_j} Y^{-1},\\
    &=x_i \ket{e_i}\bra{f_i} y_j^{-1}, \\
    &= x_i y^{-1}_j \ket{e_i}\bra{f_j},
\end{align}
which implies that $\ket{e_i}\bra{f_j}$ is an eigenmatrix of $\Delta^{X,Y}$ with eigenvalue $x_i y^{-1}_j$. Then, we have
\begin{align}
    \eta(\Delta) (\ket{e_i}\bra{f_j}) &= \eta(x_i y^{-1}_j) \ket{e_i}\bra{f_j},\\
    &= x_i y_j^{-1} (\log{x_i}-\log{y_j}) \ket{e_i} \bra{f_j}.
\end{align}
Thus we have
\begin{align}
    \Delta^{X,Y} (L_{\log{X}}-R_{\log{Y}})(\ket{e_i}\bra{f_j}) &= x_i y^{-1}_j (\log{x_i} - \log{y_j}) \ket{e_i}\bra{f_j}.
\end{align}
Because $\{\ket{e_i}\bra{f_j}\}_{i,j=1}^d$ forms a basis for $B(\mathcal{H})$, we can conclude that
\begin{align}
    \eta(\Delta^{X,Y}) &= \Delta^{X,Y}(L_{\log{X}}-R_{\log{Y}}),
\end{align}
as desired.
\end{proof}

We are now ready to prove Theorem \ref{thm:data-processing-relative-entropy}, the data processing inequality for the quantum relative entropy.

\begin{proof}
First, recall that $\text{supp}\rho \nsubseteq \text{supp}\sigma \implies D(\rho \| \sigma) := \infty.$ Next, note
\begin{enumerate}
    \item Without loss of generality, we assume that $\text{supp}\rho \subseteq \text{supp}\sigma \implies \sigma$ can be taken to be invertible.\sidenote{In general, $\mathcal{H}=\text{ker}\sigma \oplus \text{supp}\sigma$; however, we can restrict $\mathcal{H}$ to $\text{supp}{\sigma}$ by projecting.}
    \item Remember: for any quantum channel $\mathcal{N}:A\rightarrow B$, there exists and isometry $V: \mathcal{H}_A \rightarrow \mathcal{H}_B \otimes \mathcal{H}_E$ such that 
    \begin{align}
        \mathcal{N}(X_A) = \text{tr}_E(VX_A V^{\dagger})
    \end{align}
Recall that $f(V A V^{\dagger}) = V f(A) V^{\dagger}$ by definition. Applying this to the matrix logarithm yields $D(V\rho V^{\dagger} \| V \sigma V^{\dagger})=D(\rho \| \sigma)$. Then, with $\mathcal{N} \text{tr}_E V \cdot V^{\dagger}$, the claim follows from proving the data-processing inequality for $\mathcal{N}=\text{tr}_B: AB \rightarrow A$.

\item To show\sidenote{$\rho_{AB}$ must be a state but $\sigma_{AB}$ need only be positive semi-definite.}: $D(\rho_{AB} \| \sigma_{AB}) \geq D(\rho_{A}\| \sigma_A)$. Since\sidenote{Proved in Lemma B.4.1 of  \href{https://arxiv.org/pdf/quant-ph/0512258.pdf}{Renato Renner's PhD thesis.}} $\text{supp} \sigma_{AB} \subseteq \text{supp}\sigma_A \otimes \text{supp}\sigma_B $, we can assume without loss of generality that both $\sigma_{AB}$ and $\sigma_A$ are invertible. 
\end{enumerate}

Define $\Delta_{AB} = L_{\rho_{AB}} R_{\sigma_{AB}^{-1}}$ and $\Delta_A = L_{\rho_A} R_{\sigma_{A}^{-1}}$. By definition of the quantum relative entropy, we have 
\begin{align}
    D(\rho_{AB} \| \sigma_{AB}) &= \text{tr} \rho_{AB}(\log{\rho_{AB}}-\log{\sigma_{AB}}). 
\end{align}
Then,\sidenote{Remember, $\langle X,Y \rangle = \text{tr}X^{\dagger}Y$ is the inner product on $B(\mathcal{H})$.}
\begin{align}
    \eta(t)=t\log{t} \quad &= \quad \langle \sigma^{1/2}_{AB},\eta (\Delta_{AB})(\sigma_{AB}^{1/2})\rangle,\\
    &\byLemmaNine \quad \langle \sigma^{1/2}_{AB},\Delta_{AB}(L_{\log{\rho_{AB}}}-R_{\log{\sigma_{AB}}})(\sigma_{AB}^{1/2})\rangle,\\ 
    &= \quad \langle \sigma_{AB}^{1/2}, \rho_{AB} \log{\rho_{AB}} \sigma_{AB}^{1/2} \sigma_{AB}^{-1} - \rho_{AB} \sigma_{AB}^{1/2} \log{\sigma_{AB}} \sigma_{AB}^{-1} \rangle,\\
    &= \quad \text{tr}\rho_{AB} \log{\rho_{AB}}-\text{tr}\rho_{AB}\log{\sigma_{AB}},\\
    D(\rho_A \| \sigma_A)&= \quad \langle \sigma_A^{1/2}, \eta (\Delta_A)(\sigma_A^{1/2}). \rangle
\end{align}
Then, with this expression in mind, we will use the operator Jensen's inequality\sidenote{For an isometry $V$ and operator convex function $f$, $f(V^{\dagger} X V) \leq V^{\dagger} f(X) V$.}
\begin{align}
    D(\rho_{AB} \| \sigma_{AB}) \geq D(\rho_{A}\|\sigma_{A}).
\end{align}
Let us list our goals before proceeding. We wish to find $V: B(\mathcal{H}_A) \rightarrow B(\mathcal{H}_{AB})$ such that 
\begin{enumerate}
    \item $V$ is an isometry $V^{\dagger} V = \mathbb{I}_A$,
    \item $V^{\dagger} \Delta_{AB} V = \Delta_A$,
    \item $V(\sigma_A^{1/2}) = \sigma_{AB}^{1/2}$.
\end{enumerate}
Now, suppose we have found such an isometry. How would we proceed? We could then write
\begin{align}
    D(\rho_A \| \sigma_A) &= \langle \sigma_A^{1/2}, \eta(\Delta_A)(\sigma_A^{1/2})\rangle, \quad & \text{by Lemma \ref{lem:lemma-9}}\\ 
    &=\langle \sigma_A^{1/2}, \eta(V^{\dagger} \Delta_{AB} V)(\sigma_A^{1/2}) \rangle, \quad & \text{by 2)} \\
    &\leq \langle \sigma_A^{1/2},V^{\dagger} \eta(\Delta_{AB})V(\sigma_A^{1/2})\rangle, \quad & \text{operator Jensen's}\\
    &= \langle \sigma_{AB}^{1/2}, \eta(\Delta_{AB})(\sigma_{AB}^{1/2})\rangle, \quad & \text{by 3)}\\
    &=D(\rho_{AB}\| \sigma_{AB}),\quad & \text{Lemma \ref{lem:lemma-9}}
\end{align}
as desired. So, how do we choose the isometry $V$? Take 
\begin{align}
    V: X_A \mapsto (X_A \sigma_{A}^{-1/2}\otimes \mathbb{I}_B) \sigma_{AB}^{1/2},
\end{align}
where clearly $V: B(\mathcal{H}_A)\rightarrow B(\mathcal{H}_{AB})$.

\begin{enumerate}
    \item The first item is easy to check. We have $V^{\dagger} (Y_{AB}) = \text{tr}_B (Y_{AB} \sigma_{AB}^{1/2}(\sigma_A^{-1/2}\otimes \mathbb{I}_B))$, so 
    \begin{align}
        V^{\dagger}V (X_A) &= V^{\dagger} (X_A \sigma_A^{-1/2} \sigma_{AB}^{1/2}),\\
        &= \text{tr}_B(X_A \sigma_A^{-1/2} \sigma_{AB}^{1/2} \sigma_{AB}^{1/2} \sigma_A^{-1/2}), \\
        &= X_A \sigma_A^{-1/2} \sigma_A \sigma_A^{-1/2},\\
        &=X_A,
    \end{align}
    and because this holds for all $X_A$, we can conclude that $V^{\dagger} V =\mathbb{I}_A$ as desired.
    \item Next, we wish to show $V^{\dagger} \Delta_{AB} V = \Delta_A$. We have
    \begin{align}
        V^{\dagger} \Delta_{AB} V (X_A) &= V^{\dagger} \Delta_{AB} (X_A \sigma_A^{-1/2} \sigma_{AB}^{1/2}),\\
        &= V^{\dagger} (\rho_{AB} X_A \sigma_A^{-1/2} \sigma_{AB}^{1/2}\sigma_{AB}^{-1}),\\
        &=\text{tr}_B(\rho_{AB} X_A \sigma_A^{-1/2}\sigma_A^{-1/2}\sigma_A^{1/2}\sigma_A^{-1/2}),\\
        &= \text{tr}_B(\rho_{AB} X_A \sigma_A^{-1}),\\
        &= \rho_A X_A \sigma_A^{-1},\\
        &= \Delta_A (X_A),
    \end{align}
    which holds for all $X_A$, and thus we conclude  $V^{\dagger} \Delta_{AB} V = \Delta_A$ as desired. 
    
    \item $V(\sigma_A^{1/2})=\sigma_{AB}^{1/2}$
\end{enumerate}
\end{proof}
This is an extremely important result but a natural question is: does it generalize? We have shown DPI for quantum channels but Denes Petz extended these methods to prove DPIs for trace-preserving $2$-positive maps.\sidenote{$\Phi$ i $2$-positive if $\Phi \otimes \mathbb{I}_2$ is positive( $X_{AB} \geq 0 \implies (\Phi \otimes \mathbb{I})(X_A) \geq 0).$} Moreover, a very recent result by \href{https://arxiv.org/abs/1512.06117}{Mueller-Hermes and Reeb} shows that DPI holds for all trace-preserving, positive maps.\sidenote{Note that they use different proof methods based on complex interpolation.}

\chapter{Entropies and equality in data-processing} \label{ch:2-Entropies}

\section{Entropic quantities}\label{sec:entopic-quantities}
Entropies are fundamental quantities in information theory. Perhaps the most famous of which is the von Neumann entropy\sidenote{We often denote the entropy as $S(A)_{\rho} = S(\rho_A)$ where $\rho_A \in B(\mathcal{H}_A)$ is a quantum state.} defined as \index{von Neumann entropy}
\begin{align}
    S(\rho) &= -\text{tr}\rho \log{\rho} = - D(\rho \| \mathbb{I}).
\end{align}
The most useful entropic quantities also have operational interpretation. For the von Neumann entropy, there are two operational interpretations: one related to source/data compression and one to entanglement conversion of pure states. More on this later. First, let's meet some of the basic properties of the von Neumann entropy.

% Note that this counter forces us to skip statement #10. Felix seems to have skipped this in lecture and because he will likely reference these by number I am electing to match his notes.

\setcounter{theorem}{10}

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{proposition}\label{prop:properties-of-entropy} Let $S(A)_{\rho}$ be the von Neumann entropy of a quantum state $\rho_A \in B(\mathcal{H})$. Then, $S(A)_{\rho}$ has the following properties
\begin{enumerate}
    \item $0\leq S(A)_{\rho} \leq \log{|A|}$, where the first equality is reached iff $\rho_A$ is pure and the second equality is reached iff $\rho_A = \frac{1}{|A|} \mathbb{I}$
    \item Concavity: $S(\sum_i \lambda_i \rho_i) \geq \sum_i \lambda_i S(\rho_i)$
    \item Strong sub-additivity: $\forall \rho_{ABC} \in B(\mathcal{H_A} \otimes \mathcal{H}_B \otimes \mathcal{H}_C)$:
    \begin{align}
        &S(ABC)+S(C) \leq S(AC)+S(BC) \quad (A \leftrightarrow B \leftrightarrow C)\\
        &\Leftrightarrow S(A)+S(B) \leq S(AC)+S(BC), \quad \text{(weak monotonicity)}
    \end{align}
    
    Note also, that taking $|C|=1$ gives subadditivity, ($S(AB) \leq S(A) +S(B)$), from SSA.
    
    \item Let $\mathcal{N}$ be a unital quantum channel $(\mathcal{N}(\mathbb{I})=\mathbb{I})$. Then, $S(\rho) \leq S(\mathcal{N}(\rho))$ for all $\rho \in B(\mathcal{H})$. In particular, let $\{\Pi_i\}_{i=1}^k$ be a projective measurement, $(\Pi_i \geq 0, \Pi_i \Pi_j =\delta_{ij} \Pi_i, \sum_i \Pi_i =\mathbb{I})$ and $\mathcal{N}(X)=\sum_i \Pi_i X \Pi_i \implies S(\rho) \leq S(\mathcal{N}(\rho))$. In particular, $S(\rho)\leq S(\text{diag}\rho)$
\end{enumerate}
\end{proposition}
\end{tcolorbox}

\begin{proof}
\begin{enumerate}
    \item Let $\rho = \sum_i \lambda_i \ket{i}\bra{i}$ be the spectral decomposition of $\rho$. Then the entropy can be expressed as $S(\rho)=-\sum_i \lambda_i \log{\lambda_i}$. Then, because $\lambda_i \in [0,1]$, we know $S(\rho)\geq 0$. The only way to have $S(\rho)=0$, then, is if there is one eigenvalue equal to one and the rest zero. \sidenote{That is, if $\rho$ is pure $S(\rho)=0$.} 
    
    To see the upper bound on the entropy observe:
    \begin{align}
        D(\rho_A \| \frac{1}{|A|} \mathbb{I}_A) &= \text{tr}\rho_A(\log{\rho_A}-\log{\frac{1}{|A|}\mathbb{I}_A}),\\
        &=-S(\rho_A) + \log{|A|}\\
        &\geq 0,
    \end{align}
    with equality iff $\rho_A$ is maximally mixed\sidenote{We will prove this later!}.
    
    \item $S(\sum_i \lambda_i \rho_i) \geq \sum_i \lambda_i S(\rho_i)$: 
    \begin{align}
        S(\sum_i \lambda_i \rho_i) &=-D(\sum_i \lambda_i \rho_i \| \mathbb{I}),\\
        &= -D(\sum_i \lambda_i \rho_i \| \sum_i \lambda_i \mathbb{i}),\\
        &\geq \sum_i \lambda_i (-D(\rho_i \|\mathbb{I})),\\
        &= \sum_i \lambda_i S(\rho_i)
    \end{align}
    where the last property follows from the joint convexity of relative entropy shown in the proof of Proposition \ref{prop:properties-of-rel-entropy}.
    \item To show: $S(ABC) +S(C) \leq S(AC) + S(BC)$. First note
    \begin{align}
        D(\rho_{ABC} \| \rho_A \otimes \rho_{BC}) = \text{tr}\rho_{ABC}(\log{\rho_{ABC}}-\log{\rho_A \otimes \rho_{BC}}).
    \end{align}
    Then we have\sidenote{Here, we use the fact that when $[X,Y]=0$, we have $\log{XY} = \log{X}+\log{Y}$. Specifically, we have $[\rho_A \otimes \mathbb{I}_B,\mathbb{I}_A\otimes \rho_{BC}]=0$}
    \begin{align}
        \log{\rho_A \otimes \rho_{BC}} &= \log{(\rho_A \otimes \mathbb{I}_{BC})} + \log{(\mathbb{I}_A \otimes \rho_{BC})},\\
        &= \log{\rho_A} \otimes \mathbb{I}_{BC} + \mathbb{I}_A \otimes \log{\rho_{BC}},
    \end{align}
    which follows from the properties of logarithms of operators\sidenote{If this step is not immediately obvious, \href{https://math.stackexchange.com/questions/623440/logarithm-and-tensor-products}{this post} shows how one can see $\log{A\otimes \mathbb{I}} = \log{A}\otimes \mathbb{I}$ for some diagonalizable operator $A$.}
    Then, substituting back into the expression above we have 
    \begin{align}
         D(\rho_{ABC} \| \rho_A \otimes \rho_{BC}) &= \text{tr}\rho_{ABC}\log{\rho_{ABC}}\\
         &\quad- \text{tr}\rho_{ABC}(\log{\rho_A}\otimes \mathbb{I}_{BC})\\
         &\quad -\text{tr}\rho_{ABC}(\mathbb{I}_A\otimes \rho_{BC}),\\
         &=-S(ABC)+S(A)+S(BC),
    \end{align}
    then by applying data-processing with respect to $\mathcal{N}(\cdot) = \text{tr}(\cdot)$, we can conclude
    \begin{align}
        S(ABC) + S(C) \leq S(AC) + S(BC),
    \end{align}
    as desired. Note that taking $|C|=1$, we see\sidenote{Quantities that satisfy such an inequality are called sub-additive.} $S(AB)\leq S(A)+S(B).$ Next, we want to prove weak monotonocity: 
    \begin{align}
        S(A) + S(B) \leq S(AC)+S(BC).
    \end{align} To see this, let $\ket{\rho}_{ABCD}$ be a purification of $\rho_{ABC}$. By the Schmidt decomposition, $\rho_B$ and $\rho_{ACD}$ have the same spectrum\sidenote{This is a very useful fact that is often used in quantum information theory.}, which implies $S(B)=S(ACD)$ and similarly $S(BC)=S(AD)$. applying these facts, we write
    \begin{align}
        S(A) +S(ACD) &\leq S(AC) +S(AD), \\
        S(A) + S(B) &\leq S(AC) + S(BC),
    \end{align}
    as desired. 
    
    \item Finally we need to show that if $\mathcal{N}$ is unital,\sidenote{That is, $\mathcal{N}(\mathbb{I})=\mathbb{I}$.} then $S(\rho) \leq S(\mathcal{N}(\rho))$. The proof is straighforward:
    \begin{align}
        S(\rho)&= -D(\rho \| \mathbb{I}), \\
        &\leq -D(\mathcal{N}(\rho) \| \mathcal{\mathbb{I}}),\\
        &= (\mathcal{N}(\rho)),
    \end{align}
    as desired. 
\end{enumerate}
\end{proof}

Another ubiquitous quantity in quantum information theory is the conditional entropy\index{conditional entropy}
\begin{align}\label{eq:conditional-entropy}
    S(A|B)_{\rho} &= S(AB)_{\rho} -S(B)_{\rho},\\
    &= -D(\rho_{AB}\| \mathbb{I}_A \otimes \rho_B).
\end{align}
The conditional entropy has operational interpretations in terms of both the optimal rate of source compression with side information and state merging protocols in quantum Shannon theory.

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{proposition}\label{prop:properties-conditional-entropy}
\begin{enumerate}
    \item Conditioning reduces entropy: \begin{align}S(A|B)\leq S(A).
    \end{align}
    \item Duality relation: let $\rho_{AB}$ have a purification given as $\ket{\rho}_{ABC},$ then \begin{align}
        S(A|B)_{\rho} = -S(A|C)_{\rho}.
    \end{align}
    \item Range of conditional entropy: \begin{align}
        -\log{|A|}\leq S(A|B) \leq \log{|A|},
    \end{align} where the first inequality is saturated for the maximally entangled state between systems $A$ and $B$ and where the second is saturated when one system is maximally mixed.
    \item Data-processing (strong sub-additivity):
    \begin{align}
        S(A|BC)\leq S(A|B).
    \end{align}
    \item Weak monotonicity (monogomy of entanglement):
    \begin{align}
        S(A|B)+S(A|C) \geq 0.
    \end{align}
    \item Classical conditioning: Let $\rho_{AX}=\sum_x p_x \ket{x}\bra{x}_A \otimes \rho_A^x$, then
    \begin{align}
        S(A|X) =\sum_x p_x S(A)_{\rho^x}.
    \end{align}
    \item Concavity: for $\bar{\rho}=\sum_i \lambda_i \rho_{AB}^i$, we have 
    \begin{align}
        S(A|B)_{\bar{\rho}} \geq \sum_i \lambda_i S(A|B)_{\rho^i}.
    \end{align}
\end{enumerate}
\end{proposition}
\end{tcolorbox}

\begin{proof}
\begin{enumerate}
    \item $S(A|B) \leq S(A) \Leftrightarrow S(AB)-S(B)\leq S(A) $
    \item $S(A|B) =-S(A|C)$ for a state $\ket{\rho}_{ABC}$ purifying $\rho_{AB}$. By Schmidt decomposition, $S(AB)=S(C)$ and $S(B)=S(AC)$ so that 
    \begin{align}
        S(A|B) =S(AB)-S(B) =S(C)-S(AC)-S(A|C),
    \end{align}
    \item The lower bound on the range of the conditional entropy follows from the proof of 1) above and from Proposition \ref{prop:properties-of-entropy}:
    \begin{align}
        S(A|B) &= - S(A|C), \quad (\text{assume $C$ purifies $\rho_{AB}$})\\
        &\geq -S(A),\\
        &\geq -\log{|A|}.
    \end{align}
    The upper bound is shown similarly $S(A|B)\leq S(A) \leq \log{|A|}.$ The equality conditions are simple to verify. First, when $\rho_{AB}$ is separable with the $A$ system is in the maximally mixed state and the $B$ system is in any state, we have
    \begin{align}
        S(A|B)&=S(AB)-S(B),\\
        &=S(A)_{\Pi} +S(B)_{\omega} -S(B)_{\omega},\\
        &=\log{|A|}.
    \end{align}
    Lastly, we have $S(A|B)=-\log{|A|}$ when $\rho_{AB}=\Phi^+_{AB}$ with $d=|A|\leq|B|$. Because tracing out half of a maximally entangled state yields the maximally mixed state\sidenote{That is, if $\ket{\Phi^+}_{AB}=\frac{1}{\sqrt{d}} \sum_i \ket{i}_A \ket{i}_B$, then $\text{tr}_A \Phi^+_{AB}=\Pi_{B'}$, with $|B'|=d$.}, we have 
    \begin{align}
        S(A|B)&=S(AB)-S(B),\\
        &=0-\log{d},\\
        &=-\log{|A|},
    \end{align}
    as desired.
    \item Strong sub-additivity says:
    $S(ABC)+S(B)\leq S(AB)+S(CB)$, so
    \begin{align}
        S(ABC) -S(BC) \leq S(AB)-S(B),\\
        S(A|BC) \leq S(A|B).
    \end{align}
    \item Holds by weak monotonicity of von Neumann entropy\sidenote{Proven in Proposition \ref{prop:properties-of-entropy}, part 3).}
    \item $\rho_{XA}=\sum_x p_x \ket{x}\bra{x}_X \otimes \rho_A^x \implies \rho_x = \sum_x p_x \ket{x}\bra{x}$. Then, 
    \begin{align}
        S(A|X)&=-D(\rho_{XA}\|\mathbb{I}_A \otimes \rho_x),\\
        &=-D(\sum_x p_x \ket{x}\bra{x}\otimes \rho_A^x \| \mathbb{I}_A \otimes \sum_x p_x \ket{x}\bra{x}),\\
        &= -\sum_x p_x D(\rho_A^x \| \mathbb{I}_A),\\
        &=\sum_x p_x S(A)_{\rho^x},
    \end{align}
    where the third equality holds by part 6) of Proposition \ref{prop:properties-of-rel-entropy}.
    \item Concavity is shown the same way.\sidenote{Note that if $\bar{\rho}_{AB}=\sum_i \lambda_i \rho_{AB}^i,$ then $\bar{\rho}_B =\sum_i \lambda_i \rho_B^i$.} We have
    \begin{align}
        S(A|B)_{\bar{\rho}} &= -D(\sum_i \lambda_i \rho_{AB}^i \| \mathbb{I}_A \otimes \sum_i \rho_B^i),\\
        &\geq \sum_i \lambda_i (-D(\rho_{AB}^i \| \mathbb{I}_A \otimes \rho_B^i)), \\
        &=\sum_i \lambda_i S(A|B)_{\rho^i}.
    \end{align}
\end{enumerate}
\end{proof}

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{corollary}\label{cor:conditional-entropy-separable}
\begin{align}
    \rho_{AB} \quad \text{separable} \implies S(A|B)_{\rho} \geq 0
\end{align}
\end{corollary}
\end{tcolorbox}

\begin{proof}
If $\rho_{AB}$ is separable, it can be expressed as $\rho_{AB}=\sum_i \lambda_i \omega_A^i \otimes \sigma_B^i$. Then, we have\sidenote{Note: the first inequality holds by the concavity of conditional entropy that we showed in part 7) of Prop. \ref{prop:properties-conditional-entropy}.}
\begin{align}
    S(A|B)_{\rho} \geq \sum_i \lambda_i S(A|B)_{\omega^i \otimes \sigma^i},\\
    \geq 0.\\
\end{align}
Additionally, we have 
\begin{align}
    S(A|B)_{\omega^i \otimes \sigma^i} &= S(AB)-S(B),\\
    &=S(A)+S(B)-S(B),\\
    &=S(A),\\
    &\geq 0.
\end{align}
Combining these, we conclude
\begin{align}
    S(A|B)_{\rho} \geq 0,
\end{align}
as desired.\sidenote{The converse of this statement does not hold because there are \textit{bound entangled states} for which $S(A|B)_{\rho} \geq 0$. A bound entangled state is an entangled state that is undistillable. See \href{https://github.com/JacobBeckey/math595-quantumchannels}{Quantum Channels I lecture notes}.}
\end{proof}
Another well-studied entropic quantity is the \textit{coherent information}\index{coherent information} which is defined as
\begin{align}
    I_c (A>B)_{\rho} &= -S(A|B)_{\rho},\\
    &= S(B) - S(AB),\\
    &=D(\rho_{AB} \| \mathbb{I}_A \otimes \rho_B)
\end{align}
The coherent information has multiple operational interpretations:
\begin{enumerate}
    \item Entanglement distillation
    \item Quantum information transmission
    \item Quantum error correction
\end{enumerate}
Of particular note is the so-called Hashing inequality\sidenote{See for example \href{https://arxiv.org/pdf/quant-ph/0003040.pdf}{this paper} which discusses the Hashing inequality for the coherent information.}: $\rho_{AB}$ with $I_c (A>B)_{\rho} >0$ is distillable. We also note that 
\begin{align}
    S(A|B) &+ S(A|C) \geq 0\\ \Leftrightarrow I_c(A>B) &+ I(A>C)\leq 0,
\end{align}
which can be interpreted as a statement of the no-cloning theorem. 

Yet another crucially important entropic quantity is the \textit{mutual information}\sidenote{A useful way of understanding the mutual information is the relative en tropy distance from being a product state.}\index{mutual information}:
\begin{align}
    I(A;B)_{\rho} &= S(A) +S(B) -S(AB),\\
    &=S(A)-S(A|B),\\
    &=S(B)-S(B|A),\\
    &=D(\rho_{AB} \| \rho_A \otimes \rho_B).
\end{align}
It has the following operational interpretations:
\begin{enumerate}
    \item Measure for total correlations (classical and quantum) in a bipartite state 
    \item entanglement-assisted classical communication
    \item classical communication cost in state merging
\end{enumerate}
The mutual information has many important features, several of which are summarized in the following proposition.

\begin{tcolorbox}[colframe=white,breakable, colback=black!5, arc=0pt, outer arc=0pt]
\begin{proposition}\label{prop:properties-mutual-information}
\begin{enumerate}
    \item \begin{align}
        0 \leq I(A;B)_{\rho} &\leq 2\log{\min{\{|A|,|B|\}}},\\
        0\leq I(X;B)_{\rho} &\leq \log{\min{\{|X|,|B|\}}}
    \end{align}
    \item $I(A;BC) \geq I(A;B), I(AB;C)\geq I(A;C)$
    \item Holevo information: Let $\mathcal{E}=\{p_x, \rho_A^x\}$ be a quantum state ensemble. The, the Holevo information \index{Holevo information} is given as:
    \begin{align}
        \chi (\mathcal{E})&= S(\sum_x p_x \rho_A^x) - \sum_x p_x S(\rho_A^x), \\
        &= I(X;A)_{\rho},
    \end{align}
    where $\rho_{XA} =\sum_x p_x \ket{x}\bra{x}_x \otimes \rho_A^x$
    \item Holevo bound\index{Holevo bound}: Let $x\sim p(x)$ be a classical, discrete random variable, $\{\rho_B^x\}$ be a set of quantum states, and let $E=\{E_B^y\}_y$ be a POVM ($E^y \geq 0, \sum_y E^y = \mathbb{I}_B$). Denote $p(y|x)=\text{tr}(E^y_\rho \rho_B^x)$ the conditional probability distribution defining our random variable $Y, p(x,y)=p(y|x)p(x)$. Then, denote the accessible information as $I_{\text{acc}}(\{p_x,\rho_B^x\}) = \max_{E \text{ POVM}} I(X;Y)$. Then,
    \begin{align}
      I_{\text{acc}}(\{p_x,\rho_B^x\}) \leq \chi(\{p_x, \rho_B^x\}) =I(X;B)_{\rho},
    \end{align}
    where $\rho_{XB}=\sum_x p_x \ket{x}\bra{x}_x \otimes \rho_B^x$
\end{enumerate}

\end{proposition}
\end{tcolorbox}

\begin{proof}
\begin{enumerate}
    \item $0 \leq I(A;B)_{\rho} \leq 2\log{\min{\{|A|,|B|\}}}$. By part 2) of Prop \ref{prop:properties-of-rel-entropy}, we have 
    \begin{align}
        I(A;B)_{\rho} = D(\rho_{AB}\| \rho_A \otimes \rho_B) \geq 0.
    \end{align}
    We also have\sidenote{Recall that from Prop \ref{prop:properties-conditional-entropy} that $S(A|B)\geq -\log{|A|}$.}
    \begin{align}
        I(A;B) = S(A)=S(A|B) \leq S(A) + \log{|A|} \leq 2\log{|A|},
    \end{align}
    which also holds for $S(B)-S(B|A)$. Recall that $S(A|B) = -\log{|A|}$ for $\Phi_{AB}^+$ maximally entangled $(|A|\leq|B|)$. So, \begin{align}
        I(A;B)_{\Phi^+} &= S(A)+S(B)-S(AB),\\
        &=\log{|A|}+\log{|A|} - 0,\\
        &=2\log{|A|}.
    \end{align}
    When $\rho_{XA}=\sum_x p_x \ket{x}\bra{x} \otimes \rho_A^x$, we have $\quad I(X;A) \leq \log{\min{\{|X|,|A|\}}}$, so\sidenote{Recall that by part 6) of Prop. \ref{prop:properties-conditional-entropy}, $\sum_x p_x S(A)_{\rho^x} \geq 0$.}
    \begin{align}
        I(X;A) &= S(A)-S(A|X),\\
        &\leq S(A),\\
        &\leq \log{|A|},
    \end{align}
    and finally\sidenote{By Corollary \ref{cor:conditional-entropy-separable} $S(X|A)\geq 0$ because $\rho_{XA}$ is separable.}
    \begin{align}
        I(X;A) &= S(X)-S(X|A),\\
        &\leq S(X),\\
        &\leq \log{|X|}.
    \end{align}
    
    \item To show that $I(AB;C)\geq I(A;C)$, we simply apply the data-processing inequality for the channel $\text{tr}_B(\cdot)$:\begin{align}
        D(\rho_{ABC}\| \rho_{AB}\otimes \rho_C) \geq D(\rho_{AC}\|\rho_A \otimes \rho_C)= I(A;C)
    \end{align}
    \item Note that $\rho_{XA} = \sum_x p_x \ket{x}\bra{x}\otimes \rho_A^x$ implies $\rho_A=\sum_x p_x \rho_A^x$. Then,
    \begin{align}
        I(X;A) &=S(A)-S(A|X),\\
        &=S(\sum_x p_x \rho_A^x)-\sum_x p_x S(\rho_A^x),\\
    &=\chi(\{p_x,\rho_A^x\})
    \end{align}
    \item $I_{\text{acc}}(\{p_x,\rho_B^x\}) = \max_{E \quad \text{POVM}} I(X;Y) \leq I(X;B)_{\rho}$. Let our POVM, $E=\{E_B^y\}_y$, be a measurement channel.\sidenote{That is, $M(\rho)=\sum_y \text{tr}(E^y \rho) \ket{y}\bra{y}$.} Then
    \begin{align}
        I(X;B) &= D(\rho_{XB} \| \rho_X \otimes \rho_B),\\
        &=D(\sum_x p_x \ket{x}\bra{x} \otimes \rho_B^x \| \rho_X \otimes \sum_x p_x \rho_B^x),\\
        &\geq D(\sum_x p_x \ket{x}\bra{x} \otimes \sum_y p(y|x)\ket{y}\bra{y} \| \rho_x \otimes \sum_{x,y}p_x p(y|x)\ket{y}\bra{y}),\\
        &=I(X;Y) \quad (p(y,x)=p(y|x)p(x)),
    \end{align}
    which finally implies 
    \begin{align}
     I_{\text{acc}}(\{p_x, \rho_B^x\}) \leq I(X;B),
    \end{align}
    as desired.
\end{enumerate}
\end{proof}








\backmatter

%\bibliography{sample-handout}
%\bibliographystyle{plainnat}

\printindex

\end{document}
